{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Optional environment check\n",
        "from psutil import *\n",
        "print(\"Number of CPU: \", cpu_count())\n",
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "id": "zkk66V1MKok2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap output for reading more conveniently\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "-0LBdueNxACe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies not available in Colab environment and download SpaCy large dataset\n",
        "!pip install python-dotenv langchain_mistralai\n",
        "\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "P4woe7Nx4XyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "load_dotenv('/content/drive/MyDrive/Colab Notebooks/.env')\n",
        "mistral_key = os.getenv('MISTRAL_KEY')"
      ],
      "metadata": {
        "id": "NSSpG4cN4v6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-P-1P2QxmsL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "18b7b917-655d-4cfd-be61-76aa9889165e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from scipy.stats import percentileofscore\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class Grader:\n",
        "    def __init__(self, model_name = \"sentence-transformers/all-mpnet-base-v2\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "    def _get_embedding(self, text: str) -> np.ndarray:\n",
        "        inputs = self.tokenizer(text, padding=True, truncation=True,\n",
        "                              return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        token_embeddings = outputs.last_hidden_state\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sentence_embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "        return sentence_embedding.numpy()\n",
        "\n",
        "    def _string_similarity(self, str1: str, str2: str) -> float:\n",
        "      set1 = set(str1.lower())\n",
        "      set2 = set(str2.lower())\n",
        "\n",
        "      if not set1 or not set2:\n",
        "          return 0.0\n",
        "\n",
        "      intersection = len(set1.intersection(set2))\n",
        "      union = len(set1.union(set2))\n",
        "\n",
        "      return intersection / union\n",
        "\n",
        "    def _calculate_coherence(self, answer: str) -> float:\n",
        "        doc = self.nlp(answer)\n",
        "        sentences = list(doc.sents)\n",
        "\n",
        "        if len(sentences) < 2:\n",
        "            return 1.0\n",
        "\n",
        "        coherence_scores = []\n",
        "        for i in range(len(sentences) - 1):\n",
        "            sent1_embed = self._get_embedding(sentences[i].text)\n",
        "            sent2_embed = self._get_embedding(sentences[i + 1].text)\n",
        "            similarity = cosine_similarity(sent1_embed, sent2_embed)[0][0]\n",
        "            coherence_scores.append(similarity)\n",
        "\n",
        "        return np.mean(coherence_scores)\n",
        "\n",
        "    def assess_answer(self,\n",
        "                 student_answer: str,\n",
        "                 model_answer: str,\n",
        "                 rubric = None):\n",
        "\n",
        "      student_embedding = self._get_embedding(student_answer)\n",
        "      model_embedding = self._get_embedding(model_answer)\n",
        "\n",
        "      similarity_score = cosine_similarity(student_embedding, model_embedding)[0][0]\n",
        "      coherence_score = self._calculate_coherence(student_answer)\n",
        "\n",
        "      default_weights = {\n",
        "          'similarity': 0.7,\n",
        "          'coherence': 0.3,\n",
        "      }\n",
        "\n",
        "      weights = rubric.get('weights', default_weights) if rubric else default_weights\n",
        "\n",
        "      # Normalize weights to sum to 1\n",
        "      total = sum(weights.values())\n",
        "      if total > 0:\n",
        "          weights = {k: v/total for k, v in weights.items()}\n",
        "\n",
        "      final_score = (weights['similarity'] * similarity_score +\n",
        "                    weights['coherence'] * coherence_score)\n",
        "\n",
        "      return {\n",
        "          \"similarity_score\": similarity_score,\n",
        "          \"coherence_score\": coherence_score,\n",
        "          \"final_score\": final_score\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "class LLMEnhancedAssessor:\n",
        "    def __init__(self, base_assessor, api_key, model = \"mistral-large-latest\"):\n",
        "\n",
        "        self.base_assessor = base_assessor\n",
        "        self.model = model\n",
        "        self.llm = ChatMistralAI(\n",
        "            model_name=model,\n",
        "            mistral_api_key=api_key,\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "    async def _generate_llm_analysis(self, student_answer, benchmark_answer, base_assessment):\n",
        "\n",
        "        analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are an AI tutor who interprets assessment scores. Use a conversational second-person tone without any formatting (no bold, italic, or other text formatting). Analyze the student's answer compared to the benchmark, focusing on conceptual understanding. Identify specific strengths, point out areas needing improvement, and offer practical suggestions without referencing specific study materials. Use no section headers, greetings, or introductory paragraphs. You can use bullet points when necessary. Keep your responses concise and conversational. Do not mention the similarity or coherence scores. Do not compare the student answer to benchmark directly. Do not mention the benchmark answer.\"),\n",
        "            (\"user\", \"\"\"\n",
        "            I've received scores on my assessment and would like your help understanding my performance and how to improve.\n",
        "\n",
        "            The benchmark answer was: {benchmark_answer}\n",
        "            My answer was: {student_answer}\n",
        "\n",
        "            Answer scores:\n",
        "            - Similarity to benchmark answer: {similarity_score}\n",
        "            - Coherence: {coherence_score}\n",
        "\n",
        "            Can you analyze my work, highlight my strengths and areas for improvement, and suggest what I should focus on to improve?\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = analysis_prompt | self.llm\n",
        "\n",
        "        result = await chain.ainvoke({\n",
        "            \"benchmark_answer\": benchmark_answer,\n",
        "            \"student_answer\": student_answer,\n",
        "            \"similarity_score\": base_assessment['similarity_score'],\n",
        "            \"coherence_score\": base_assessment['coherence_score']\n",
        "        })\n",
        "\n",
        "        return result\n",
        "\n",
        "    async def generate_feedback(self, student_answer, benchmark_answer):\n",
        "\n",
        "        rubric = {\n",
        "        'weights': {\n",
        "          'similarity': 0.7,\n",
        "          'coherence': 0.3,\n",
        "      }\n",
        "    }\n",
        "\n",
        "        base_result = self.base_assessor.assess_answer(\n",
        "            student_answer, benchmark_answer, rubric\n",
        "        )\n",
        "\n",
        "        llm_analysis = await self._generate_llm_analysis(\n",
        "            student_answer,\n",
        "            benchmark_answer,\n",
        "            base_result,\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"score\": base_result,\n",
        "            \"tutor_feedback\": llm_analysis.content,\n",
        "        }"
      ],
      "metadata": {
        "id": "b82suA6y2T7J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d4d38854-2139-431b-f58a-77f49e2b9d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_assessor = Grader()\n",
        "\n",
        "rubric = {\n",
        "        'weights': {\n",
        "          'similarity': 0.7,\n",
        "          'coherence': 0.3,\n",
        "      }\n",
        "    }\n",
        "\n",
        "enhanced_assessor = LLMEnhancedAssessor(\n",
        "        base_assessor=base_assessor,\n",
        "        api_key=mistral_key\n",
        "    )\n",
        "\n",
        "benchmark_answer =\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "student_answer =\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "result = await enhanced_assessor.generate_feedback(\n",
        "        student_answer=student_answer,\n",
        "        benchmark_answer=benchmark_answer,\n",
        "    )\n",
        "\n",
        "print(f\"{result['score']['similarity_score']:.2f} {result['score']['coherence_score']:.2f} {result['score']['final_score']:.2f}\\n\")\n",
        "print(result['tutor_feedback'])"
      ],
      "metadata": {
        "id": "2D8-hVh12nMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}